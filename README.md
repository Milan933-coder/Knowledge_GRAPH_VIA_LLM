ğŸš€ LLM-Driven Knowledge Graph Pipeline
Ontology Engineering â€¢ Knowledge Extraction â€¢ Entity Resolution

An end-to-end experimental pipeline for building high-quality Knowledge Graphs (KGs) using small/efficient LLMs with dynamic memory handling, ontology discovery, and Neo4j-based entity resolution.

ğŸ“Œ Motivation

Building Knowledge Graphs from large corpora using LLMs faces three major challenges:

â— Context window limitations

â— Duplicate entity creation

â— Weak ontology consistency

â— Lack of dynamic memory across chunks

This project explores a modular, research-oriented pipeline that addresses these issues through:

âœ… Ontology Engineering with LLMs
âœ… Structured Knowledge Extraction
âœ… Vector-based Entity Resolution
âœ… Neo4j Graph Storage
âœ… Dynamic memory strategies

ğŸ§  High-Level Pipeline
flowchart LR
    A[Raw Corpus / PDF] --> B[Chunking]
    B --> C[Ontology Discovery]
    C --> D[Ontology Refinement]
    D --> E[Knowledge Extraction]
    E --> F[Entity Resolution]
    F --> G[Neo4j Graph]
    G --> H[Post-processing & Analysis]

ğŸ—ï¸ Project Architecture
.
â”œâ”€â”€ ontology_extractor.py        # Ontology discovery & refinement
â”œâ”€â”€ kg_llm_extractor.py          # LLM-based knowledge extraction
â”œâ”€â”€ neo4j_entity_resolution.py   # Vector-based entity deduplication
â”œâ”€â”€ .env                         # Neo4j credentials
â””â”€â”€ README.md

ğŸ”¬ Core Components
1ï¸âƒ£ Ontology Engineering

File: ontology_extractor.py

ğŸ¯ Goal

Automatically discover and refine the schema before KG construction.

ğŸ§© Approach

Phase 1 â€” Ontology Discovery

The LLM analyzes corpus chunks to identify:

Candidate entity types

Relationship types

Attribute patterns

Domain concepts

Phase 2 â€” Ontology Refinement

The discovered ontology is:

Deduplicated

Normalized

Structured into a consistent schema

Validated for conflicts

âœ… Why This Matters

Without ontology grounding:

KG becomes noisy

Relations become inconsistent

Downstream reasoning fails

This step provides schema stability before extraction.

2ï¸âƒ£ Knowledge Extraction

File: kg_llm_extractor.py

ğŸ¯ Goal

Convert unstructured text into structured triples.

âš™ï¸ Extraction Strategy

For each chunk:

Pass chunk to small LLM

Extract structured triples

Attach metadata (chunk_id, confidence, etc.)

Store intermediate results

ğŸ§± Output Format
{
  "entities": [...],
  "relationships": [...],
  "source_chunk": "...",
  "confidence": 0.xx
}

ğŸ”‘ Key Design Decisions
âœ… Chunk-aware extraction

Each triple retains provenance.

âœ… Schema-guided prompting

Extraction is constrained by discovered ontology.

âœ… Small-model friendly

Designed to run with lightweight models (e.g., Mistral-7B).

3ï¸âƒ£ Entity Resolution (Deduplication)

File: neo4j_entity_resolution.py

This is one of the most critical innovations in the pipeline.

ğŸš¨ Problem

When processing chunks independently:

Same entity appears multiple times

Graph becomes fragmented

Query quality degrades

ğŸ’¡ Solution

We implement vector-based entity resolution using:

Sentence embeddings

Similarity search

Neo4j vector index

Threshold-based merging

ğŸ”„ Resolution Flow
flowchart TD
    A[New Entity] --> B[Generate Embedding]
    B --> C[Search Similar Entities]
    C --> D{Similarity > Threshold?}
    D -->|Yes| E[Merge Entities]
    D -->|No| F[Create New Node]

ğŸ§  Matching Strategy

We compare using:

Name similarity

Semantic embedding similarity

Optional attribute matching

This significantly reduces duplicate nodes.

ğŸ§© Dynamic Memory Strategy

One major discussion during development was:

â“ How do we prevent the LLM from "forgetting" previous chunks?

âŒ Naive Approach

Process chunks independently â†’ leads to:

duplicate entities

inconsistent relations

ontology drift

âœ… Our Hybrid Solution

We combine:

ğŸ”¹ Pre-Extraction Memory

Ontology grounding

Schema constraints

ğŸ”¹ Post-Extraction Memory

Entity resolution

Graph merging

Community clustering

ğŸš€ Why This Works Better

Instead of forcing the LLM to remember everything (which is expensive and unreliable), we:

âœ” Let the model work locally
âœ” Fix globally via graph algorithms
âœ” Maintain scalability

ğŸ—„ï¸ Neo4j Integration

The graph layer provides:

Persistent memory

Efficient traversal

Vector similarity search

Graph analytics

ğŸ”§ Required Environment Variables

Create a .env file:

NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_password

â–¶ï¸ How to Run
1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

2ï¸âƒ£ Start Neo4j

Make sure Neo4j is running locally.

3ï¸âƒ£ Run Ontology Discovery
python ontology_extractor.py

4ï¸âƒ£ Run Knowledge Extraction
python kg_llm_extractor.py

5ï¸âƒ£ Run Entity Resolution
python neo4j_entity_resolution.py

ğŸ“Š Design Philosophy

This project follows several important principles:

ğŸ§  LLMs are not memory systems

We avoid overloading context windows and instead use:

Graph memory

Vector search

Post-processing

âš¡ Small models > giant models (for pipelines)

The system is optimized for:

Mistral-7B class models

Kaggle/consumer GPUs

Efficient inference

ğŸ”„ Graph post-processing is essential

High-quality KGs require:

deduplication

clustering

schema enforcement

â€”not just extraction.

ğŸ§ª Future Work

 Online ontology adaptation

 Streaming KG construction

 Community detection integration

 Temporal knowledge graphs

 Multi-agent extraction

 GraphRAG integration

ğŸ¤ Contributions

Contributions, ideas, and research discussions are welcome!

If you are working on:

KG construction

GraphRAG

Ontology learning

LLM pipelines

feel free to open an issue or PR.

â­ Acknowledgment

This project is an experimental research effort exploring the intersection of:

Knowledge Graphs

Small Language Models

Graph Databases

Representation Learning

ğŸ’¬ Author Note

Building robust Knowledge Graphs with LLMs is not just an extraction problem â€”
it is a systems engineering problem involving memory, ontology, and graph intelligence.
